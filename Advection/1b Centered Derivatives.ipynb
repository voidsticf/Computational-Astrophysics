{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b Centered Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can approximate partial spatial derivatives with finite differences, and consider the function values $f_{i,jk}$ at discrete points $(i\\Delta x,j\\Delta y,k\\Delta z)$ in three dimension to be variables whose time evolution is governed by ordinary differential equations (ODEs) in time. Using for example a 2nd order finite difference to approximate the spatial derivatives, and looking at a case with constant velocity ${\\bf v}$ and a profile $f(r,t)$ that is time independent in the moving coordinate frame ($f({\\bf r}_0-{\\bf v}t,t) = f({\\bf r}_0,0)$), we will need to compute spatial derivatives, with formulae such as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\tag{1}\n",
    "\\left(\\frac{\\partial f}{\\partial x}\\right)_{i,j,k} = \\frac{1}{2\\Delta x} \\left(f_{i+1,j,k}-f_{i-1,j,k}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finite difference approximation for the derivative can be derived by considering Taylor series of the function (and therefore assumes that the function is well behaved). We can write:\n",
    "\\begin{align}\\tag{2}\n",
    "f_{i+1,j,k} & = f_{i,j,k} + \\left(\\frac{\\partial f}{\\partial x}\\right)_{i,j,k} \\Delta x + {1 \\over 2} \\left(\\frac{\\partial^2 f}{\\partial x^2}\\right)_{i,j,k} \\Delta x^2 + \\mathcal{O}(\\Delta x^3) \\\\\n",
    "f_{i-1,j,k} & = f_{i,j,k} - \\left(\\frac{\\partial f}{\\partial x}\\right)_{i,j,k} \\Delta x + {1 \\over 2} \\left(\\frac{\\partial^2 f}{\\partial x^2}\\right)_{i,j,k} \\Delta x^2 + \\mathcal{O}(\\Delta x^3)\n",
    "\\end{align}\n",
    "Subtracting the two to remove $f_{i,j,k}$ and rearranging terms we see that in the limit where $\\Delta x$ becomes small the error, or difference, between the finite difference equation and the correct value for the derivative should scale like $\\Delta x^2$. This is called a second order approximation. Notice that this is an asymptotic scaling; a priori we do not know how large the constant of this second error is, and it entirely depends on the smoothness of the function involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Python to play around with this, we need a function that returns the spatial derivative in each of the three directions.  To avoid having to specify boundary conditions we assumme that the domain is periodic.  Then we can use the `numpy` function `roll` to quickly and efficiently compute the finite differences.\n",
    "\n",
    "While we could put import and the derivative function together, it is good practice to have a seperate cell at the top that imports the neccessary modules. Further down, we will need to do plotting. Therefore, we also import the `matplotlib.pyplot` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv(f,ds,axis=0):\n",
    "    \"\"\" \n",
    "    Compute derivative in the axis-direction, given f[i,j,k] \n",
    "    and ds = mesh size in the axis-direction\n",
    "    \"\"\"\n",
    "    return (np.roll(f,-1,axis)-np.roll(f,+1,axis))/(2.0*ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy.roll` (or `np.roll`, because we import `numpy` as `np`) does a \"circular shift\" (shifting with periodic wrapping). To see documentation for it, do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.roll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try the partial derivative on a sin(x) function we create a 3D array with a `sin(x)` function along the x-axis.\n",
    "\n",
    "Notice that we put a `;` on the last line, this will prevent Jupyter in automatically trying to print the return value from the last function, which in this case will just be some info about what and where things have been plotted.\n",
    "\n",
    "If you get annoyed by an output window with scroll bars then right-click anywhere on the notebook window and select _Disable scrolling for outputs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinx_3d(m):\n",
    "    f=np.zeros(m)\n",
    "    x=np.zeros(m[0])\n",
    "    ds=2.0*np.pi/np.array(m)\n",
    "    for i in range(m[0]):\n",
    "        x[i]=i*ds[0]\n",
    "        f[i,:,:]=np.sin(x[i])\n",
    "    return f,x,ds\n",
    "n=128\n",
    "m=(n,n,n)\n",
    "f,x,ds=sinx_3d(m)\n",
    "plt.plot(x,f[:,0,0])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the derivative, which should be $cos(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdx=deriv(f,ds[0],0)\n",
    "plt.plot(x,dfdx[:,0,0],label='numerical derivative')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\partial f/\\partial x$')\n",
    "plt.plot(x,np.cos(x),'+',label='analytical derivative')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `deriv` function also works for 1-D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=np.sin(x)\n",
    "dx=ds[0]\n",
    "dfdx=deriv(f,dx)\n",
    "plt.plot(x,f);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the size of the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdx=deriv(f,dx)\n",
    "plt.plot(x,dfdx,label='derivative')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\partial f/\\partial x$')\n",
    "plt.plot(x,1e3*(dfdx-np.cos(x)),label='error x1000')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, the error in the finite difference derivative is a very small amount (less than one part in a thousand), with the given resolution (128 points in 2$\\pi$). We can also see that the error is largest where the curvature of the derivative is largest.   \n",
    "\n",
    "With lower resolution, the error grows, of course.  How do you think it scales with the resolution?  You can check your answer by modifying the code above (or copying the relevant cells below here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in (8,16,32,64,128):\n",
    "    dx=2.*np.pi/n\n",
    "    x=dx*np.arange(n)\n",
    "    f=np.sin(x)\n",
    "    d=max(abs(np.cos(x)-deriv(f,dx)))\n",
    "    print('n: {:3d}  error: {:.6f}'.format(n,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the error for 2nd order derivatives, and make a loglog plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=[]; d2=[]\n",
    "for n in (8,16,32,64,128):\n",
    "    dx=2.*np.pi/n\n",
    "    x=dx*np.arange(n)\n",
    "    f=np.sin(x)\n",
    "    nn.append(n)\n",
    "    d=max(abs(np.cos(x)-deriv(f,dx)))\n",
    "    d2.append(d)\n",
    "plt.loglog(nn,d2,'o',linestyle=':')\n",
    "plt.xlabel('n'); plt.ylabel('error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch out! In the above example I have defined the error as the maximum of the absolute difference between the analytical derivative and the finite difference approximation. How to define the error is somewhat arbitrary; one could also define it as the typical difference or even the RMS (second order) norm. The important part is to specify what has been done, and sometimes also understand why we get the results we get. E.g. a certain method could give excellent results except for at specific points. A maximum norm, as used above, is sensitive to outlier points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Compare with 4th order centered derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for a 4th order centered derivative is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\tag{2}\n",
    "\\left(\\frac{\\partial f}{\\partial x}\\right)_{i,j,k} = \n",
    "\\frac{1}{12 \\Delta x} \n",
    "\\left[\n",
    "8(f_{i+1,j,k}-f_{i-1,j,k})\n",
    "-(f_{i+2,j,k}-f_{i-2,j,k})\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be shown be expanding in Taylor series for the four different points and see how terms cancel. Notice that while the error term scales as $(\\Delta x)^4$, the prefactor is the fifth derivative of the function. Therefore approximating the derivative of a function with sudden changes -- e.g. a step function -- may result in large errors.\n",
    "\n",
    "__Task:__ Compute the corresponding errors for the 4th order derivatives, and plot them as filled squares (Python symbol `'s'`), together with the 2nd order errors as filled circles (symbol `'o'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bonus task:__ Try to increase the number of points from 128 to 1e3, 1e4, 1e5, 1e6, and 1e7 for 2nd and 4th order. Do you understand what goes on with the error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Absalon turn in:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   1. Replace the ellipses (...) in the following sentence with the 4th order result, and add the code to the notebook below this cell.\n",
    "   2. Upload the notebook as a pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_So, while the error at `n=128` is less than a part in a thousand for 2nd order, it's less than ... with 4th order! The error scales as ... for 4th order_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
